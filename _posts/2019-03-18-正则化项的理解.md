---
title: 正则化项的理解
layout: post
categories: 面试
excerpt: 
Tags: 面试
---

#### 机器学习中正则化项L1和L2的直观理解

参考：<https://blog.csdn.net/jinping_shi/article/details/52433975>

L1、L2正则化：

L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为$||w||_1$

L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$||w||_2$

**下面是L1正则化和L2正则化的作用**:

- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择
- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

假设有如下带L1正则化的损失函数：
$$
J=J_0+\alpha\sum_w|w|
$$
![image](https://ws1.sinaimg.cn/large/006tKfTcly1g178ud2luhj308u089js0.jpg)





图中等值线是$$J_0​$$ 的等值线，黑色方形是L函数的图形。在图中，当$$J_0​$$等值线与L图形首次相交的地方就是最优解。因为L函数有很多突出的角（二维情况下四个，多维情况下更多）,J0与这些角接触的机率会远大于与L其它部位接触的机率这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。

#### L2正则化和过拟合

拟合过程中通常都倾向于**让权值尽可能小**，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。

**那为什么L2正则化可以获得值很小的参数？**

没有正则化项的用于迭代计算参数$$\theta_j$$的形式
$$
\theta_j:=\theta_j-\alpha\frac{1}m\sum_{i=1}^m(h_\theta(x^i)-y^i)x_j^i
$$

$$
\theta_j:=\theta_j(1-\alpha\frac{\lambda}m)-\alpha\frac{1}m\sum_{i=1}^m(h_\theta(x^i)-y^i)x_j^i
$$


与未添加L2正则化的迭代公式相比，每一次迭代，θj 都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的











