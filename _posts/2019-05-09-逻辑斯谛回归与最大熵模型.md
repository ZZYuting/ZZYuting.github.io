---
title: 逻辑斯谛回归与最大熵模型
layout: post
categories: 机器学习基础
excerpt: 
Tags: 机器学习基础
---

#### 逻辑斯谛回归

逻辑回归模型是由以下条件概率分布(二项分布)表示的分类模型，
$$
P(Y=k|x)=\frac{exp(w_k*x)}{1+\sum_{k=1}^{K-1}exp(w_k*x)},k=1,2,...K-1
$$

$$
P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}exp(w_k*x)}
$$

逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以**实际中最常用的就是二分类的逻辑回归。**

**仅能用于线性问题**。只有当**目标和特征是线性关系时**，才能用逻辑回归。在应用逻辑回归时注意两点：**一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征**。

#### 逻辑回归与朴素贝叶斯有什么区别？

1. 逻辑回归是**判别模型**， 朴素贝叶斯是**生成模型**，所以生成和判别的所有区别它们都有。
2. **朴素贝叶斯属于贝叶斯，逻辑回归是最大似然**，两种概率哲学间的区别。
3. 朴素贝叶斯**需要条件独立假设**。
4. 逻辑回归**需要求特征参数间是线性的**。

#### 最大熵原理

最大熵原理认为在所有可能的概率模型(分布)的集合中，**熵最大的模型是最好的模型**

### 逻辑回归为什么使用对数损失函数？

假设逻辑回归模型
$$
P(y=1|x;\theta)=\frac{1}{1+e^{-\theta^{T}x}}
$$
假设**逻辑回归模型的概率分布是伯努利分布**，其概率质量函数为：
$$
P(X=n)=
\begin{cases}
1-p, n=0\\
 p,n=1
\end{cases}
$$
其似然函数为：
$$
L(\theta)=\prod_{i=1}^{m}
P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}
$$
对数似然函数为：
$$
\ln L(\theta)=\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]\\
  =\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]
$$
对数函数在单个数据点上的定义为：
$$
cost(y,p(y|x))=-y\ln{p(y|x)-(1-y)\ln(1-p(y|x))}
$$
则全局样本损失函数为：
$$
cost(y,p(y|x)) = -\sum_{i=1}^m[y_i\ln p(y_i|x_i)+(1-y_i)\ln(1-p(y_i|x_i))]
$$
由此可看出，**对数损失函数与极大似然估计的对数似然函数本质上是相同的**。所以逻辑回归直接采用对数损失函数。